\documentclass[10pt, oneside]{article}   	
\usepackage[left=25mm,top=25mm,right=25mm,bottom=25mm]{geometry}   
\usepackage[toc,page]{appendix}
\geometry{a4paper}
\usepackage{algorithm,algpseudocode}  
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{pgfplots}					
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{gensymb}
\usepackage{multirow}
\usepackage{url}
\usepackage{titlesec}
\usepackage{subcaption}
\usepackage[export]{adjustbox}
\usepackage[parfill]{parskip}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{array}
\usepackage{siunitx}
\usepackage{listings}
\usepackage{fixltx2e}
\usepackage{color}
\usepackage{diagbox}
\usepackage[parfill]{parskip}
\usepackage{pythonhighlight}
\setlength{\headsep}{5pt}
\graphicspath{ {images/} }
\lstset{
  basicstyle=\ttfamily,
  columns=fixed,
  fontadjust=true,
  basewidth=0.5em
}
\setcounter{topnumber}{4}
\setcounter{bottomnumber}{1}
\setcounter{totalnumber}{3}
\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
  \OLDthebibliography{#1}
  \setlength{\parskip}{0pt}
  \setlength{\itemsep}{1pt plus 0.5ex}
}
\renewcommand\thesection{\alph{section}}
\renewcommand\thesubsection{\thesection.\roman{subsection}}
\titleformat{\section}
{\normalfont\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
{\normalfont\normalsize\bfseries}{\thesubsection}{1em}{}
\title{\vspace{-1cm}Biomedical Information Processing (R214): Main Assignment}
\author{Chongyang Shi \emph{(cs940)}}
\date{\today}							
\begin{document}
\maketitle

For the main course assignment, I am undertaking the second practical option (\textbf{1.2}): \emph{extracting chemical-disease associations from the biological literature}.

\section{Improving the Conditional Random Fields named entity recognizer} \label{sec:a-crf-features}
\subsection{Ablating features from the original feature set} \label{subsec:ablating}

Based on the default \emph{n}-gram feature set in the feature extraction script, the script was modified to ablate each feature in turn. To provide a better understanding of the effects by offsets of surface form words, all surface forms of words within an offset of 1 (the trigram) were knocked out from the templates first, then just the surface forms of words before and after the current word (-1/1). Other features including the lemma, phonetic coding ($soundex$), part-of-speech, and chunk in IOB2 notation of the current word only. The resulting precisions, recall rates, and $F_1$-scores from ablating each feature on the \emph{devel} dataset are presented separately in Figures \ref{fig:ablation1}, \ref{fig:ablation2}, and \ref{fig:ablation3}. For each named entity class as well as the overall average, with none-ablated as reference, improved performance due to ablation are presented in \textbf{bold}.

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|*{8}{c|}}\hline
\backslashbox{Class}{Ablated} & None & $word$ (all) & $word$ (-1/1) & $lemma$ & $soundex$ & $pos$ & $chunk$ \\ \hline
B-Chemical & 0.9178 & \textbf{0.9345} & \textbf{0.9409} & 0.9056 & 0.9015 & \textbf{0.9495} & \textbf{0.9210} \\ \hline
O                 & 0.9560 & 0.9471 & 0.9498 & 0.9540 & 0.9531 & 0.9499 & 0.9557 \\ \hline
B-Disease   & 0.8403 & 0.8242 & 0.8223 & \textbf{0.8418} & 0.8387 & \textbf{0.8412} & 0.8396 \\ \hline
I-Disease    & 0.7404 & 0.7152 & 0.7167 & \textbf{0.7467} & \textbf{0.7506} & \textbf{0.7631} & \textbf{0.7509} \\ \hline
I-Chemical  & 0.7556 & 0.6488 & 0.6745 & \textbf{0.7569} & \textbf{0.7612} & \textbf{0.7906} & \textbf{0.7682} \\ \hline
\textbf{Macro-average} & 0.8420 & 0.8142 & 0.8209 & 0.8410 & 0.8410 & \textbf{0.8589} &\textbf{ 0.8471} \\ \hline
\end{tabular}
\caption{\label{fig:ablation1} Resulting \textbf{precisions} on different named entity classes from ablating individual features from the original feature set. }
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|*{8}{c|}}\hline
\backslashbox{Class}{Ablated} & None & $word$ (all) & $word$ (-1/1) & $lemma$ & $soundex$ & $pos$ & $chunk$ \\ \hline
B-Chemical & 0.6664 & 0.5583 & 0.5955 & 0.6564 & 0.6520 & 0.5702 & 0.6652 \\ \hline
O                 & 0.9888 & 0.9888 & \textbf{0.9889} & 0.9888 & 0.9887 & \textbf{0.9908} & \textbf{0.9894} \\ \hline
B-Disease   & 0.6011 & 0.5514 & 0.5672 & 0.5669 & 0.5561 & 0.5806 & 0.5992 \\ \hline
I-Disease    & 0.6018 & 0.5530 & 0.5607 & 0.5993 & 0.5952 & \textbf{0.6029} & 0.5952 \\ \hline
I-Chemical  & 0.5961 & 0.5114 & 0.5275 & 0.5950 & 0.5910 & 0.5938 & \textbf{0.5990} \\ \hline
\textbf{Macro-average} & 0.6908 & 0.6326 & 0.6479 & 0.6813 & 0.6766 & 0.6677 & 0.6896 \\ \hline
\end{tabular}
\caption{\label{fig:ablation2} Resulting \textbf{recall rates} on different named entity classes from ablating individual features from the original feature set. }
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|*{8}{c|}}\hline
\backslashbox{Class}{Ablated} & None & $word$ (all) & $word$ (-1/1)& $lemma$ & $soundex$ & $pos$ & $chunk$ \\ \hline
B-Chemical & 0.7721 & 0.6992 & 0.7294 & 0.7611 & 0.7567 & 0.7125 & \textbf{0.7725} \\ \hline
O                 & 0.9721 & 0.9675 & 0.9690 & 0.9711 & 0.9706 & 0.9699 & \textbf{0.9723} \\ \hline
B-Disease   & 0.7008 & 0.6607 & 0.6713 & 0.6776 & 0.6687 & 0.6870 & 0.6993 \\ \hline
I-Disease    & 0.6640 & 0.6238 & 0.6292 & \textbf{0.6649} & 0.6639 & \textbf{0.6736} & \textbf{0.6641} \\ \hline
I-Chemical  & 0.6665 & 0.5720 & 0.5920 & 0.6662 & 0.6654 & \textbf{0.6782} & \textbf{0.6731} \\ \hline
\textbf{Macro-average} & 0.7551 & 0.7046 & 0.7182 & 0.7451 & 0.7451 & 0.7443 & \textbf{0.7562} \\ \hline
\end{tabular}
\caption{\label{fig:ablation3} Resulting \textbf{$F_1$-scores} on different named entity classes from ablating individual features from the original feature set.}
\end{center}
\end{figure}

Surprisingly, for chemicals at the beginning of entities (B-Chemcal), the precision (correct tags among those tagged) increased substantially when the surface form word feature is ablated. Ablating only the surface forms of before and after tokens (-1/1) produced slightly higher precision than ablating the entire surface form trigram. This is however accompanied by substantially reduced precisions on all other named entity classes, as well as reduced recall rate (correct tags among all relevant inputs that can be tagged) nearly across the board. As B-chemicals already bear a fairly high precision (91.78\%), it is not advisable to ablate the surface forms and reduce recall into the 50\% range. 

Ablation of the lemma (base word) and the phonetic coding ($soundex$) yielded minimal improvements to precisions on some named entity groups but minimal reductions on others. Recall rates all reduced by very small margins. Based on a generally negative outlook on the $F_1$-scores (combined metric of precision and recall), it is advisable not to alate either of he two features.

Ablating the part-of-speech produced the greatest precision improvements to most groups, but mostly lowered the recall rate substantially. This is also reflected in the overall negative outlook on the combined $F_1$-scores. Therefore it is not advisable to ablate the part-of-speech feature. 

Finally, ablating the chunk information from the feature set improved the precision without significantly affecting the recall rate in most cases, resulting in improved $F_1$-scores for all named entity classes barring diseases within entities (I-Disease) with a minimal decrease. Therefore, it is advisable to ablate the chunk information from the feature set used.

Vertically, precision and recall rates of terms outside entities (O) are high and only very minimally affected by ablating any of the features, which is generally expected in entity recognition operations due to the abundance of outside tokens between short named entities \cite{ratinov2009design}.

\subsection{Improvements to the base tagger}

After removing chunk information to improve performance of the base feature set (as described above), I will first experiment with expanding the \emph{n}-gram feature set by expanding unigram features into trigram features. Then, I will examine the effects of adjusting several parameters of the L-BFGS training algorithm used in \emph{crfsuite}.

\subsubsection{Expansion of unigram features}

Evaluations of word representation features in entity recognition by Tang et al. \cite{tang2014evaluating} demonstrated the benefits of using trigram features in word stemming. With this as inspiration, I iteratively expanded the unigram features of lemma, part-of-speech, and the phonetic coding ($soundex$) in the feature set. The order of expansion was chosen due to lemma being directly related to word stemming, part-of-speech correlations between neighbouring words normally being important, and the phonetic coding being the one left. The resulting performance information are shown in Figure \ref{fig:trigrams}.

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|*{7}{c|}}\hline
\textbf{Expanded from unigram}  & \multicolumn{3}{c|}{None} & \multicolumn{3}{c|}{$lemma$}  \\ \hline 
\textbf{Entity Class} & Precision & Recall & $F_1$-score & Precision & Recall & $F_1$-score  \\ \hline
B-Chemical & 0.9210 & 0.6652 & 0.7725 & 0.9137 & 0.6695 & 0.7728 \\ \hline
O                 & 0.9557 & 0.9894 & 0.9723 & 0.9559 & 0.9890 & 0.9722\\ \hline
B-Disease   & 0.8396 & 0.5992 & 0.6993 & 0.8365 & 0.5992 & 0.6982 \\ \hline
I-Disease    & 0.7509 & 0.5952 & 0.6641 & 0.7519 & 0.6040 & 0.6699 \\ \hline
I-Chemical  & 0.7682 & 0.5990 & 0.6731 & 0.7820 & 0.6013 & 0.6798 \\ \hline
\textbf{Macro-average} & 0.8471 & 0.6896 & 0.7562 & 0.8480 & 0.6926 & 0.7586 \\ \hline \hline
\textbf{Expanded from unigram}  & \multicolumn{3}{c|}{$lemma$ + $pos$} & \multicolumn{3}{c|}{$lemma$ + $pos$ + $soundex$} \\ \hline 
\textbf{Entity Class} & Precision & Recall & $F_1$-score & Precision & Recall & $F_1$-score  \\ \hline
B-Chemical & 0.9077 & 0.6864 & 0.7817 & 0.9077 & 0.6875 & 0.7824 \\ \hline
O                 & 0.9574 & 0.9894 & 0.9731 & 0.9574 & 0.9894 & 0.9731 \\ \hline
B-Disease   & 0.8499 & 0.6162 & 0.7144 & 0.8477 & 0.6124 & 0.7111 \\ \hline
I-Disease    & 0.7819 & 0.6103 & 0.6855 & 0.7795 & 0.6110 & 0.6850 \\ \hline
I-Chemical  & 0.7884 & 0.6138 & 0.6903 & 0.8010 & 0.6241 & 0.7016 \\ \hline
\textbf{Macro-average} & 0.8570 & 0.7032 & 0.7690 & 0.8587 & 0.7049 & 0.7706 \\ \hline
\end{tabular}
\caption{\label{fig:trigrams} Resulting tagging performance on the \emph{devel} dataset after expanding unigram features into trigram features in the baseline feature set. ``None'' represents the baseline feature set with $chunk$ ablated.}
\end{center}
\end{figure}

While the precision of B-Chemical tagging continues to follow the declining trend discussed in \ref{subsec:ablating} (although not as severe in unigram expansion as in feature ablation), expanding unigram features of lemma, part-of-speech, and the phonetic coding into trigrams have resulted in improved or roughly equal precisions, recall rates -- and hence $F_1$-scores. Precision improvements were most notable from the additions of lemma and phonetic coding on chemicals within entities (I-Chemical), showing the influence of phonetic features of nearby entities on chemical entity recognition. The strongest improvement of recall and the $F_1$-score originated from expanding part-of-speech to nearby entities, demonstrating the importance of expanding the semantic scope. While precisions of some named entities took a small hit when the phonetic coding ($soundex$) was added, the improved macro-average precision as well as generally improved recall rates and $F_1$-scores were behind my choice of retaining all three unigram expansions.

\subsubsection{Adjustment of training parameters}

The L-BFGS training algorithm used in entity recognition uses L2 regularization, which is normally more efficient than L1 \cite{cortes20092}, with a regularization parameter influencing the levels of bias and overfitting. Various values around the default $c2 = 1$ were tested, with broadly similar trends in variations of precision, recall, and $F_1$-score in individual named entities. Therefore, only the macro-average metrics from tagging the \emph{devel} dataset with a model trained on each value of $c2$ are plotted in Figure \ref{fig:c2}.

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    xlabel={c2 Regularization Parameter},
    ylabel={Performance},
    xmin=0, xmax=2.5,
    ymin=0, ymax=1.0,
    xtick={0, 0.5, ..., 2.5},
    ytick={0, 0.1, ..., 1.0},
    legend pos=south east,
    ymajorgrids=true,
    grid style=dashed,
]
 
\addplot[
color=blue,
mark=x,
]
coordinates {
(0.05,0.780505)(0.1,0.782937)(0.25,0.781873)(0.5,0.777822)(1.0,0.770645)(2.0,0.753415)
};
\addplot[
color=red,
mark=x,
]
coordinates {
(0.05,0.730290)(0.1,0.730579)(0.25,0.724635)(0.5,0.715561)(1.0,0.704877)(2.0,0.682223)
};
\addplot[
color=green,
mark=x,
]
coordinates {
(0.05,0.843873)(0.1,0.848921)(0.25,0.855177)(0.5,0.859233)(1.0,0.858653)(2.0,0.853710)
};
\legend{$F_1$-score,Recall,Precision}
\end{axis}
\end{tikzpicture}
\caption{\label{fig:c2} Resulting tagging performance on the \emph{devel} dataset with models trained on different c2 values.}
\end{center}
\end{figure}

A reduced $c2$ parameter results in longer training, but also generally improved recall and the overall $F_1$-score. With the adverse effects of extended training time and reduced precision (due to increased overfitting), as well as diminished gains in recall, $c2=0.25$ was chosen as the $c2$ value of choice.

With $c2=0.25$, L-BFGS also allows different line search algorithms, which yield roughly the same training speed, as well as almost the same performance with all named entity classes, as shown in Figure \ref{fig:linesearch}. Therefore the default More and Thuente's method is kept unchanged.

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|*{10}{c|}}\hline
\textbf{Line Search}  & \multicolumn{3}{c|}{$MoreThuente$} & \multicolumn{3}{c|}{$Backtracking$} & \multicolumn{3}{c|}{$StrongBacktracking$}  \\ \hline 
\textbf{Entity Class} & Precision & Recall & $F_1$-score & Precision & Recall & $F_1$-score & Precision & Recall & $F_1$-score \\ \hline
B-Chemical & 0.9223 & 0.6972 & 0.7941 & 0.9223 & 0.6972 & 0.7941 & 0.9223 & 0.6972 & 0.7941 \\ \hline
O                 & 0.9609 & 0.9884 & 0.9745 & 0.9609 & 0.9884 & 0.9744 & 0.9609 & 0.9884 & 0.9744 \\ \hline
B-Disease   & 0.8363 & 0.6671 & 0.7422 & 0.8353 & 0.6656 & 0.7409 & 0.8352 & 0.6661 & 0.7411 \\ \hline
I-Disease    & 0.7515 & 0.6338 & 0.6876 & 0.7520 & 0.6330 & 0.6874 & 0.7520 & 0.6330 & 0.6874 \\ \hline
I-Chemical  & 0.8048 & 0.6367 & 0.7110 & 0.8048 & 0.6367 & 0.7110 & 0.8048 & 0.6367 & 0.7110 \\ \hline
\textbf{Macro-average} & 0.8552 & 0.7246 & 0.7819 & 0.8550 & 0.7242 & 0.7816 & 0.8550 & 0.7243 & 0.7816 \\ \hline
\end{tabular}
\caption{\label{fig:linesearch} Resulting tagging performance on the \emph{devel} dataset with different line search algorithms: More and Thuente's method, backtracking method with regular Wolfe condition, and backtracking method with strong Wolfe condition.}
\end{center}
\end{figure}

\subsection{Evaluate the improved model on the test set}

To summarise, the changes made on the baseline entity recognizer include the ablation of chunk information from the feature set during feature extraction; expanding lemma, part-of-speech, and phonetic coding ($soundex$) features from unigrams into trigrams (covering feature information of entities before and after the current word); as well as adjusting L-BFGS' $c2$ regularization parameter to 0.25. A comparison between the improved model and the original model is shown in Figure \ref{fig:improved-compare}.

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|*{7}{c|}|c|}\hline
\textbf{Model}  & \multicolumn{3}{c|}{\textbf{Improved}} & \multicolumn{3}{c||}{\textbf{Original}} & \emph{Change} \\ \hline 
\textbf{Entity Class} & Precision & Recall & $F_1$-score & Precision & Recall & $F_1$-score & $F_1$-score \\ \hline
B-Chemical & 0.9168 & 0.6771 & 0.7789 & 0.9085 & 0.6457 & 0.7549 & +3.18\% \\ \hline
O                 & 0.9619 & 0.9890 & 0.9753 & 0.9576 & 0.9882 & 0.9726 & +0.28\% \\ \hline
B-Disease   & 0.8218 & 0.6514 & 0.7268 & 0.8217 & 0.5895 & 0.6865 & +5.87\% \\ \hline
I-Disease    & 0.7522 & 0.6434 & 0.6936 & 0.7311 & 0.6178 & 0.6697 & +3.57\% \\ \hline
I-Chemical  & 0.7998 & 0.6087 & 0.6913 & 0.7438 & 0.6081 & 0.6691 & +3.32\% \\ \hline
\textbf{Macro-average} & 0.8505 & 0.7139 & 0.7732 & 0.8325 & 0.6899 & 0.7506 & +3.01\% \\ \hline
\end{tabular}
\caption{\label{fig:improved-compare} A comparison between the improved model and the original entity recognition model on tagging performance on the \emph{test} dataset.}
\end{center}
\end{figure}

Changes made to features and L-BFGS parameters resulted in a model which yields improved precision and recall across the board. Combining the metrics of precision and recall, changes in $F_1$-scores are shown on the right of Figure \ref{fig:improved-compare}. The already high-precision and high-recall terms outside entities received the least improvement, while disease terms at the start of entities benefited the most from the improved model. More than 3\% of improvements were made to all other named entity classes and the macro-average. Based on these results from testing on the unseen \emph{test} input dataset, I believe it is reasonably conclusive that the improved model is superior to the original in entity recognition of environment and disease terms.

\section{Grounding named entities through approximate string matching}

The MESH concept dictionary provides terms for two classes of entities: chemicals and diseases. Based on the class of entity determined by the improved entity recognition model from the previous section, all possible choices of the specified class in the MESH dictionary were supplied to the approximate string matching process facilitated by the \verb|fuzzywuzzy| \cite{fuzzywuzzy} library. With the initial approach, best matching dictionary terms were grouped by their original sentences, which were annotated with their respective matched terms in output.

Unfortunately, this initial approach clearly appeared to be inadequate due to the lack of connecting recognised terms belonging to a continuous entity:

\begin{lstlisting}[breaklines]
**Urine**{1} **N**{2} **-**{3} **acetyl**{4} **-**{5} **beta**{6} **-**{7} **D**{8} **-**{9} **glucosaminidase**{10} - - a marker of **tubular**{11} **damage**{12} ?
{1} purine (Score: 91)
{2} alanine (Score: 90)
{3} 11-deoxycortisol (Score: 0)
{4} 2-acetylaminofluorene (Score: 90)
{5} 11-deoxycortisol (Score: 0)
{6} 17beta-estradiol (Score: 90)
{7} 11-deoxycortisol (Score: 0)
{8} 1,2-DMH (Score: 90)
{9} 11-deoxycortisol (Score: 0)
{10} AMI (Score: 90)
{11} acute tubular necrosis (Score: 90)
{12} axonal damage (Score: 90)
\end{lstlisting}

The first ten terms in the above sentence included in the \emph{devel} dataset are designated as being outside a named entity class (``O''), but mis-recognised by the entity recognition model as a chemical spanning across multiple terms (in fact, the assembled term is an enzyme, which can be counted as a chemical but not considered as such by the dataset's context). The result of the fragmented individual approximate string matching process is a long list of poor matches annotated. Similarly, the fragmentation of ``tabular'' and ``damage'' caused them to be individually (and inaccurately) matched with individual terms in the dictionary.

These two errors highlight the need of reassembling neighbouring terms belonging to the same entity before attempting approximate string matching. Therefore, the grounding process was modified to first reassemble these neighbouring terms together (e.g. from ``... + $O$ + $B-Chemical$ + $I-Chemical$ + $I-Chemical$ +  $O$ + ...'' to ``... + $O$ + $Chemical$ +  $O$ + ...'') before matching the assembled term with the best approximation in the dictionary. The above example now becomes:

\begin{lstlisting}[breaklines]
**Urine N - acetyl - beta - D - glucosaminidase ** {1} - - a marker of **tubular damage ** {2} ? 
{1} 9-[[2-methoxy-4-[(methylsulphonyl)amino]phenyl]amino] -N,5-dimethyl- 4-acridinecarboxamide (Score: 86)
{2} acute tubular necrosis (Score: 86)
\end{lstlisting}

While the enzyme is still not matched by an appropriate term in the dictionary (likely simply does not exist), with reassembly applied, ``acute tubular necrosis'' is now a very good biomedical description of ``tubular damage'' in the original text. To further resolve the lack of dictionary coverage over complex chemical constructs, efficient co-occurrence-based concept associations \cite{tsuruoka2008facta} could aid the grounding system in finding the best-matching entity based on functional groups in the term. 

Overall, the grounding method with entity reassembly works fairly well, such as on the followed sentence:

\begin{lstlisting}[breaklines]
BACKGROUND : **Calcitriol ** {1} therapy suppresses serum levels of parathyroid hormone ( PTH ) in patients with **renal failure ** {2} but has several drawbacks , including **hypercalcemia ** {3} and / or marked suppression of bone turnover , which may lead to adynamic bone disease . 
{1} Ca (Score: 90)
{2} renal failure (Score: 100)
{3} hypercalcemia (Score: 100)
\end{lstlisting}

While Calcitriol does not exist in the MESH dictionary, it does increase the body's intake of its closest match in the dictionary -- calcium (Ca). Although this is mostly a lucky match due to the lack of a less relevant term with a shorter edit distance, similar processes of inferring terms through biomedical relations have already been applied elsewhere to improve grounding, such as utilising contrastive information between proteins \cite{kim2005biocontrasts}. More systematically, machine learning-based inference systems trained on biomedical databases can used to effectively construct knowledge base from unstructured biomedical information \cite{shin2015incremental}. In the above example, ``renal failure'' was also correctly matched with the corresponding term in the dictionary after entity reassembly before grounding.

Some other issues do persist after entity reassembly, such as the lack of ability to match complex acronyms with their full base words:

\begin{lstlisting}[breaklines]
(Simple acronyms)
CBA / **Ca ** {1} male mice started on **AZT ** {2} 0 . 75 mg / ml **H2O ** {3} at 84 days of age and kept on it for 687 days when dosage reduced to 0 . 5 mg / ml **H2O ** {4} for a group , another group removed from **AZT ** {5} to see recovery , and third group remained on 0 . 75 mg . 
{1} Ca (Score: 100)
{2} AZT (Score: 100)
{3} H2O (Score: 100)
{4} H2O (Score: 100)
{5} AZT (Score: 100)

(Complex, lesser-known acronyms)
RESULTS : In Nx dogs , **OCT ** {1} significantly decreased serum PTH levels soon after the induction of **renal insufficiency ** {2} . 
{1} methoctramine (Score: 90)
{2} renal insufficiency (Score: 100)
\end{lstlisting}

From the original literature \cite{monier199922}, ``OCT'' refers to Oxacalcitriol, which while nevertheless not in the dictionary, was incorrectly interpreted as methoctramine. It is possible to resolve most acronyms into canonical forms through fixed or dynamic rules based on ontology knowledge \cite{naderi2011organismtagger}.

Finally, common proportions of biomedical composite words and multi-word nouns can lead to incorrect groundings of tagged terms when a direct match with the dictionary vocabulary does not occur. For example:

\begin{lstlisting}[breaklines]
Histological examination on 9 of 10 mice with such **thrombocytopenia ** {1} showed changes compatible with **myelodysplastic syndrome ** {2} ( **MDS ** {3} ) . 
{1} thrombocytopenia (Score: 100)
{2} Fanconi syndrome (Score: 86)
{3} DES (Score: 67)
\end{lstlisting}

Myelodysplastic syndromes concern bone marrows, while Fanconi syndrome describes kidney conditions. With the nature of the conditions entirely changed, this result from grounding is entirely erroneous. Methods to better distinguish semantic compositions of compound words (``compound bracketing'') through unsupervised probabilistic models \cite{pecina2010lexical}, as well as CRF post-processing and lexicon/dictionary-supported normalization \cite{lee2016audis} have been developed to improve accuracy when grounding compound words and multi-word nouns.

In general, a further improved grounding system may have the following features:
\begin{itemize}
	\item Co-occurrence-based concept associations that can be queried efficiently \cite{tsuruoka2008facta} are used to find best-matching entities of complex chemical constructs if a direct match in vocabulary does not exist;
	\item For sources containing unstructured biomedical information, databases storing relational information \cite{kim2005biocontrasts} between entities are used to train inference agents \cite{shin2015incremental} to better resolve terms without a direct match;
	\item To resolve uncommon acronyms that are not present in the dictionary, rules generated from ontology databases are applied to resolve the acronyms into canonical forms \cite{naderi2011organismtagger}, which are more likely to encounter good matches during grounding;
	\item Unsupervised probabilistic models \cite{pecina2010lexical} and CRF-based normalization algorithms \cite{lee2016audis} are used to improve accuracy when grounding compositions of terms.
\end{itemize}

\section{Identifying associations between disease and chemical mentions}

The full abstract collection of PubMed texts on chemically induced disorders numbered 301,084,933 lines, with words already processed into a format identical to those used in Section \ref{sec:a-crf-features}. With reference named entity classes unavailable for each surface word (defaulting to outside entities), the improved entity recognition model from Section \ref{sec:a-crf-features} was used to perform entity recognition on a total of 301,084,933 surface words from 10,573,978 sentences. Tags generated through entity recognition were then cross-referenced with the original surface words and their lemmas for grounding, which was conducted in parallelised batches to reduce memory footprint. 

With grounded entities grouped by the sentences they originated from, duplicate entities within the same sentence were removed, with the assumption that multiple mentions of a noun entity in the same sentence is primarily for clarity rather than emphasis \cite{cortes20092}. Grounded entities in the sentence were then sorted in alphanumeric order to preserve consistency in composing co-mention pairs. The mention of each entity in each sentence was recorded globally as an occurrence of that entity. All possible in-order combinations of entities of length 2 were then generated with Python's \verb|itertools.combinations|. Based on the context of the study, an additional filter was placed so that only co-mention pairs of a chemical and a disease are recorded. Implementations of these pre-processing steps can be found in Figure \ref{fig:preprocessing} of Appendix \ref{sec:implementations}.

By the number of occurrences, the ten most common grounded entities and the pairs thereof in the PubMed abstract texts are shown in Figure \ref{fig:popular-individuals} and Figure \ref{fig:popular-pairs}.

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|*{3}{c|}p{1.9cm}||*{3}{c|}p{1.9cm}|}\hline
\multicolumn{4}{|c||}{\textbf{Chemicals}} & \multicolumn{4}{c|}{\textbf{Diseases}} \\ \hline 
Count & Probability & MESH ID & Name & Count & Probability & MESH ID & Name \\ \hline 
248227 & 0.045\% & D006859 & H & 166561 & 0.030\% & D009369 & tumour \\ \hline 
135662 & 0.024\% & D009569 & NO & 155455 & 0.028\% & D004714 & endometrial hyperplasia or cancer \\ \hline 
102140 & 0.018\% & D008694 & METH & 128998 & 0.023\% & D064420 & Toxicity \\ \hline 
79642 & 0.014\% & C034818 & methyl 6,7-dimethoxy-4-ethyl-B-carboline-3-carboxylate & 60568 & 0.011\% & D020511 & disorder of neuromuscular transmission \\ \hline 
78439 & 0.014\% & C066430 & 3-aminopropyl-diethoxy-methyl-phosphinic acid & 58780 & 0.011\% & D007239 & infections \\ \hline 
70656 & 0.013\% & C025136 & phenylacetic acid & 54426 & 0.010\% & D047508 & massive hepatocellular necrosis \\ \hline 
68107 & 0.012\% & D004298 & Dopamine & 51521& 0.009\% & D003643 & deaths \\ \hline 
65758 & 0.012\% & D018698 & glutamine & 44556 & 0.008\% & D012140 & respiratory and cardiovascular depression \\ \hline 
63797 & 0.011\% & D005947 & glucose & 40723 & 0.007\% & D031901 & gestational trophoblastic disease \\ \hline 
62878 & 0.011\% & D002118 & Calcium & 38994 & 0.007\% & D008103  & cirrhosis of the liver \\ \hline 
\end{tabular}
\caption{\label{fig:popular-individuals} The top ten most commonly mentioned chemical and disease entities by appearance in number of sentences.}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|*{6}{c|}}\hline
\multicolumn{2}{|c|}{\textbf{Association}} & \multicolumn{2}{c|}{\textbf{Chemical}} & \multicolumn{2}{c|}{\textbf{Disease}} \\ \hline 
Count & Probability & MESH ID & Name & MESH ID & Name \\ \hline 
6425 & 0.00116\% & D003404 & creatinine & D009369 & tumour \\ \hline
5860 & 0.00105\% & D003404 & creatinine & D047508 & massive hepatocellular necrosis \\ \hline
5180 & 0.00093\% & D006859 & H & D020511 & disorder of neuromuscular transmission \\ \hline
3995 & 0.00072\% & D004967 & estrogen & D004714 & endometrial hyperplasia or cancer \\ \hline
3850 & 0.00069\% & D005472 & 5-FU & D004714 & endometrial hyperplasia or cancer \\ \hline
3259 & 0.00059\% & D006859 & H & D012140 & respiratory and cardiovascular depression \\ \hline
3162 & 0.00057\% & D002945 & cisplatinum & D004714 & endometrial hyperplasia or cancer \\ \hline
3112 & 0.00056\% & D004317 & Doxorubicin & D004714 & endometrial hyperplasia or cancer \\ \hline
2986 & 0.00054\% & D006859 & H & D004714 & endometrial hyperplasia or cancer \\ \hline
2902 & 0.00052\% & D002945 & cisplatinum & D009369 & tumour \\ \hline
\end{tabular}
\caption{\label{fig:popular-pairs} The top ten most common associations of chemical and disease entities by appearance in number of sentences.}
\end{center}
\end{figure}

For the purpose of calculating occurrence probabilities, all sentences with any mention of a chemical or disease are counted into the total, even if a pair of chemical and disease cannot be established within the sentence. This is to maintain the consistency between chemical occurrences and disease occurrences. In addition to occurrences of individual and pairs of entities, four statistical measure are considered: Pointwise Mutual Information (PMI), Normalized Pointwise Mutual Information (NPMI), the Jaccard coefficient/index, and Symmetric Conditional Probability (SCP). With occurrences of pairs of chemical and disease entities recorded in dictionaries indexed by 2-tuples of IDs, calculations of these statistical occurrence metrics for pair of entities are straightforward through dictionary comprehensions, as shown in Figure \ref{fig:stats-calc} of Appendix \ref{sec:implementations}. Top ten associations as measured by these metrics are shown in Figures \ref{fig:pmi}, \ref{fig:npmi}, \ref{fig:jaccard}, and \ref{fig:scp}.

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|*{2}{c|}p{4.5cm}|c|p{4.5cm}|}\hline
 \multirow{2}{*}{\textbf{PMI}} & \multicolumn{2}{c|}{\textbf{Chemical}} & \multicolumn{2}{c|}{\textbf{Disease}} \\ \cline{2-5}
 & MESH ID   & Name                      & MESH ID   & Name                         \\ \hline
 8.95518 & C476217   & cinacalcet HCl            & D006961   & hyperparathyroidism          \\ \hline
 8.72841 & D005702   & Galanthamine hydrobromide & D014826   & vocal fold palsy             \\ \hline
 8.35901 & D013390   & Suxamethonium chloride    & D005207   & Fasciculations               \\ \hline
 8.35375 & D011441   & Propylthiouracil          & D006980   & hyperthyroidism              \\ \hline
 8.31867 & D013390   & Suxamethonium chloride    & D012019   & reflex sympathetic dystrophy \\ \hline
 8.31393 & C031942   & argatroban                & D013684   & telangiectasis               \\ \hline
 8.16299 & D005013   & ethosuximide              & D004832   & absence seizures             \\ \hline
 8.07865 & D008972   & molindone                 & D002819   & Choreoathetoid movements     \\ \hline
 7.99780 & D007464   & clioquinol                & C538178   & acrodermatitis enteropathica \\ \hline
 7.83060 & D004025   & dicyclomine               & D004211   & intravascular coagulation    \\ \hline
\end{tabular}
\caption{\label{fig:pmi} The top ten associations of chemical and disease entities as measured by Pointwise Mutual Information (PMI).}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|*{2}{c|}p{4.5cm}|c|p{4.5cm}|}\hline
 \multirow{2}{*}{\textbf{NPMI}} & \multicolumn{2}{c|}{\textbf{Chemical}} & \multicolumn{2}{c|}{\textbf{Disease}} \\ \cline{2-5}
  & MESH ID   & Name                      & MESH ID   & Name                         \\ \hline 
 0.594802 & D002248   & carbon monoxide    & D011041   & poisoning                                   \\ \hline
 0.550872 & C476217   & cinacalcet HCl     & D006961   & hyperparathyroidism                         \\ \hline
 0.535089 & D014673   & vecuronium bromide & D020879   & neuromuscular blockade                      \\ \hline
 0.530970 & D010622   & phencyclidine      & D006996   & hypocalcemia                                \\ \hline
 0.521261 & D004025   & dicyclomine        & D004211   & intravascular coagulation                   \\ \hline
 0.515581 & D018170   & sumatriptan        & D008881   & Migraine                                    \\ \hline
 0.507942 & D007980   & Levodopa           & D055154   & dysphonia                                   \\ \hline
 0.501047 & C010012   & adriamycinone      & D000160   & adverse effect on the proximal eighth nerve \\ \hline
 0.491024 & D002996   & clomiphene citrate & D011085   & polycystic ovary syndrome                   \\ \hline
 0.490998 & D011441   & Propylthiouracil   & D006980   & hyperthyroidism                             \\ \hline
\end{tabular}
\caption{\label{fig:npmi} The top ten associations of chemical and disease entities as measured by Normalized Pointwise Mutual Information (NPMI).}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|*{2}{c|}p{4.5cm}|c|p{4.5cm}|}\hline
 \multirow{2}{*}{\textbf{SCP}} & \multicolumn{2}{c|}{\textbf{Chemical}} & \multicolumn{2}{c|}{\textbf{Disease}} \\ \cline{2-5}
 & MESH ID   & Name                      & MESH ID   & Name                         \\ \hline
 0.0396361  & D002248   & carbon monoxide    & D011041   & poisoning                       \\ \hline
 0.0177305  & D007980   & Levodopa           & D055154   & dysphonia                       \\ \hline
 0.0144888  & D010622   & phencyclidine      & D006996   & hypocalcemia                    \\ \hline
 0.0111154  & D003404   & creatinine         & D047508   & massive hepatocellular necrosis \\ \hline
 0.00975593 & D013311   & streptozotocin     & D003920   & Diabetic                        \\ \hline
 0.00924036 & D014673   & vecuronium bromide & D020879   & neuromuscular blockade          \\ \hline
 0.00903808 & D000244   & ADP                & D001791   & platelet aggregations           \\ \hline
 0.00865604 & C047426   & venlafaxine        & D001281   & Atrial Fibrillation             \\ \hline
 0.00822348 & D018170   & sumatriptan        & D008881   & Migraine                        \\ \hline
 0.00799549 & D007538   & Isoniazid          & D014376   & tuberculosis                    \\ \hline
\end{tabular}
\caption{\label{fig:scp} The top ten associations of chemical and disease entities as measured by Symmetric Conditional Probability (SCP).}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|*{2}{c|}p{4.5cm}|c|p{4.5cm}|}\hline
 \multirow{2}{*}{\textbf{Jaccard}} & \multicolumn{2}{c|}{\textbf{Chemical}} & \multicolumn{2}{c|}{\textbf{Disease}} \\ \cline{2-5}
 & MESH ID   & Name                      & MESH ID   & Name                         \\ \hline
        0.594802 & D002248   & carbon monoxide    & D011041   & poisoning                                   \\ \hline
        0.550872 & C476217   & cinacalcet HCl     & D006961   & hyperparathyroidism                         \\ \hline
        0.535089 & D014673   & vecuronium bromide & D020879   & neuromuscular blockade                      \\ \hline
        0.530970  & D010622   & phencyclidine      & D006996   & hypocalcemia                                \\ \hline
        0.521261 & D004025   & dicyclomine        & D004211   & intravascular coagulation                   \\ \hline
        0.515581 & D018170   & sumatriptan        & D008881   & Migraine                                    \\ \hline
        0.507942 & D007980   & Levodopa           & D055154   & dysphonia                                   \\ \hline
        0.501047 & C010012   & adriamycinone      & D000160   & adverse effect on the proximal eighth nerve \\ \hline
        0.491024 & D002996   & clomiphene citrate & D011085   & polycystic ovary syndrome                   \\ \hline
        0.490998 & D011441   & Propylthiouracil   & D006980   & hyperthyroidism                             \\ \hline
\end{tabular}
\caption{\label{fig:jaccard} The top ten associations of chemical and disease entities as measured by the Jaccard index.}
\end{center}
\end{figure}

\bibliographystyle{IEEEtran}
\small{\bibliography{report}}
\begin{appendices}
\section{Implementation excerpts from statistical processing of grounded entities.} \label{sec:implementations}
\begin{figure}[h]
\inputpython{../grounding/co_occurrences.py}{32}{69}
\caption{\label{fig:preprocessing} Pre-processing of grounded entities in sentences separated by new lines.}
\end{figure}

\begin{figure}[h]
\inputpython{../grounding/co_occurrences.py}{107}{112}
\caption{\label{fig:stats-calc} Calculations of statistical occurrence metrics for pairs of chemical and disease entities.}
\end{figure}

\end{appendices}

\end{document}  
